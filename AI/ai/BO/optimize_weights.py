# optimize_weights.py

"""
1. ê°•í™” í•™ìŠµ (RL), 2. ë² ì´ì§€ì•ˆ ìµœì í™” (BO)

ê¸°ì¡´ ì½”ìŠ¤ë¥¼ ê°•í™”í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼,
ì½”ìŠ¤ í‰ê°€ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ í†µí•´ weightë¥¼ ê°•í™”í•™ìŠµì‹œí‚¤ëŠ” êµ¬ì¡°

ì½”ìŠ¤ ìƒì„±ì€ ê¸°ì¡´ API ë‚´ë¶€ì—ì„œ ìˆ˜í–‰
result_evalë§Œ ê°€ì§€ê³  BO/RLì„ ëŒë¦¼
BOëŠ” ë‹¨ìˆœíˆ â€œì´ result_evalì´ë©´ ê°€ì¤‘ì¹˜ê°€ ì–¼ë§Œí¼ ì¢‹ì•„ì•¼ í•œë‹¤â€ë¥¼ í•™ìŠµ
BOê°€ ì¶œë ¥í•œ paramsëŠ” ë‹¤ìŒ API í˜¸ì¶œì—ì„œ ì‚¬ìš©í•˜ëŠ” weightë¡œ ì£¼ì…

"""


from .agent_bo import BOAgent
from .reward import reward_fn
from .rl_runner import CourseRL
from .param_storage import load_context_params, save_context_params

RESULT_NUM = 7
STORE_PATH = "./best_params.json"

# -------------------------------
# ë²”ì£¼í™”(Binning) í•¨ìˆ˜
# -------------------------------
def bin_sensitivity(value):
    if value is None:
        return "none"
    if value <= 3:
        return "LOW"
    if value <= 6:
        return "MID"
    return "HIGH"
def n_day_sensitivity(value):
    if value is None:
        return "none"
    if value <= 2:
        return "SHORT"
    if value <= 5:
        return "MID"
    return "LONG"

# -------------------------------
# í‚¤ ìƒì„±: region + DIS_BIN + POP_BIN + n_day
# -------------------------------
def make_context_key(user_context):
    region = "_".join(user_context.get("region", [])) if isinstance(user_context.get("region"), list) else user_context.get("region", "none")
    
    dist = bin_sensitivity(user_context.get("distance_sensitivity"))
    pop  = bin_sensitivity(user_context.get("popular_sensitivity"))
    nday = n_day_sensitivity(user_context.get("n_day", "none"))

    return f"{region}_{dist}_{pop}_{nday}"


def safe_reward_fn(result_eval, user_context):
    # í•„ìˆ˜ key ì—†ìœ¼ë©´ reward=0 ë°˜í™˜
    required = ["place_score_avg_list", "geo_score_list", "diversity_score", "popular_scores_list"]
    for k in required:
        if k not in result_eval or result_eval[k] is None:
            return 0

    try:
        reward = reward_fn(result_eval, user_context)
        if np.isnan(reward) or reward is None:
            return 0
        return reward
    except:
        return 0



#def optimize_weights():
def optimize_weights(result_eval, user_context):
    """      
        result_eval = {
            "place_score_avg_list":place_score_avg_list,
            "geo_score_list":geo_score_list,
            "diversity_score":div_score,
            "popular_scores_list":popular_scores_list,            
        }
        user_context.update({
            "region": region_list,
            "select_list": select_list,
            "distance_sensitivity": distance_sensitivity,
            "popular_sensitivity": popular_sensitivity,
            "n_day": n_day,
            "transit": transit,
            "bandwidth": bandwidth,
            "enough_place": enough_place,
        })

    """
    # (1) result_eval ì²´í¬
    if not result_eval:
        print("[WARN] empty result_eval â†’ skip")
        return None

    # (2) key ìƒì„±
    key = make_context_key(user_context)

    # (3) ì €ì¥ëœ best ê°€ì ¸ì˜¤ê¸°
    stored = load_context_params(key)
    if stored:
        best_reward = stored["best_reward"]
        best_params = stored["params"]
    else:
        best_reward = float("-inf")
        best_params = None

    # (4) BO + RL ì‹¤í–‰
    dimensions = [...]
    agent = BOAgent(dimensions)
    rl = CourseRL(safe_reward_fn)

    history = rl.run(agent, result_eval, user_context, episodes=40)

    # ìƒˆ best ì°¾ê¸°
    new_best = agent.best()
    if not new_best:
        return best_params  # fallback

    new_reward = new_best["reward"]
    new_params = new_best["params"]

    # (5) ë¹„êµ í›„ ì €ì¥ ì—¬ë¶€ ê²°ì •
    if new_reward > best_reward:
        print("ğŸ‰ Improved weights â†’ save")
        save_context_params(key, new_params, new_reward)
        return new_params
    else:
        print("ğŸ˜ No improvement â†’ keep previous")
        return best_params
